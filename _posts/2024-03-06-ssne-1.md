---
layout: post
title:  "Neural nets WUT notes"
author: "Mikołaj Szawerda"
tags: ML
---

## Algorytmy z normalizacją gradientów

### Adagrad

- modyfikacja algorytmu SGD
- Efektywny parametr kroku $\frac{\beta}{\sqrt{G+\epsilon}}$ - wartości nierosnące
- zawsze stabilny

### RMSprop

- modyfikacja SGD
- wygładzanie wykładnicze
- ogranicza zmniejszenie agresywnego zmniejszania wag z adagrad

### Adadelta

- modyfikacja RMSprop
- dodatkowo akumulacja historii kwadratów zmian wag

### Adam

- akumuluje kwadraty i ma korekty estymatorów