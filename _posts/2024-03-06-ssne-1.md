---
layout: post
title:  "Neural nets WUT notes"
author: "Mikołaj Szawerda"
tags: ML
excerpt_separator: <!--more-->
---

Notes taken during Neural Nets course on WUT

<!--more-->

## Algorytmy z normalizacją gradientów

### Adagrad

- modyfikacja algorytmu SGD
- Efektywny parametr kroku $\frac{\beta}{\sqrt{G+\epsilon}}$ - wartości nierosnące
- zawsze stabilny

### RMSprop

- modyfikacja SGD
- wygładzanie wykładnicze
- ogranicza zmniejszenie agresywnego zmniejszania wag z adagrad

### Adadelta

- modyfikacja RMSprop
- dodatkowo akumulacja historii kwadratów zmian wag

### Adam

- akumuluje kwadraty i ma korekty estymatorów

## Przeuczenie

- wczesne zatrzymanie - dopóki strata na walidacyjnym spada
- regularyzacja - karanie za duże wartości wag 
  - L1 - zazwyczaj wybierają tylko część nerueonów, zerując pozostałe
- dropout - z prawdopodobieństwem $p$ zerouje wyjście
  - sprawia ze wiedza staje się rozproszona w całej sieci, przez co model uodpornia się na poszczególne wartości wag/neuronów
- augmentacja danych
- destylacja wiedzy
  - najpierw stworzenie wielkiego modelu, a potem transfer wiedzy do mniejeszego modeli
- inicjowanie wag - Xavier
- transfer learning
- normalizacja pakietowa - batch normalization
- wyznaczanie niepewności
  - epistemiczna - niewystarczająca ilość wiedzy
  - aletoryczna - spowodowana naturalną losowością zagadnienia
- ball tree - wzorce aktywacji